{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自訂的輸入層及辨識層(Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟1：載入 data (with mask & without mask) 資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 images belonging to 2 classes.\n",
      "Found 1376 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path='D:\\Python\\Python_3 機器學習應用開發 (主修)\\Class 8\\Homework\\HW3 CNN 神經網路實作_V2\\Lab 2\\data'\n",
    "test_path='D:\\Python\\Python_3 機器學習應用開發 (主修)\\Class 8\\Homework\\HW3 CNN 神經網路實作_V2\\Lab 2\\data'\n",
    "\n",
    "\n",
    "train_batches=ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_path,\n",
    "        color_mode='grayscale',\n",
    "        target_size=(150, 150),\n",
    "        shuffle=True,\n",
    "        classes=['with_mask','without_mask'])\n",
    "            \n",
    "test_batches=ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(150, 150),\n",
    "        shuffle=True,\n",
    "        color_mode='grayscale',\n",
    "        classes=['with_mask','without_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 64)      640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               752760    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 242       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,012,010\n",
      "Trainable params: 1,012,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs=30\n",
    "width,height=150,150\n",
    "num_features=64\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(num_features,(3,3),activation='relu',input_shape=(width,height,1)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(num_features,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(2*num_features,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(2*num_features,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(120,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18804\\235262591.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history=model.fit_generator(train_batches,epochs=25,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "43/43 [==============================] - 53s 1s/step - loss: 0.6293 - accuracy: 0.6265 - val_loss: 0.4087 - val_accuracy: 0.8256\n",
      "Epoch 2/25\n",
      "43/43 [==============================] - 55s 1s/step - loss: 0.2721 - accuracy: 0.8866 - val_loss: 0.1590 - val_accuracy: 0.9324\n",
      "Epoch 3/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.1666 - accuracy: 0.9390 - val_loss: 0.0881 - val_accuracy: 0.9688\n",
      "Epoch 4/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0893 - accuracy: 0.9666 - val_loss: 0.1035 - val_accuracy: 0.9644\n",
      "Epoch 5/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0703 - accuracy: 0.9731 - val_loss: 0.0469 - val_accuracy: 0.9797\n",
      "Epoch 6/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0379 - accuracy: 0.9855 - val_loss: 0.0221 - val_accuracy: 0.9913\n",
      "Epoch 7/25\n",
      "43/43 [==============================] - 59s 1s/step - loss: 0.0493 - accuracy: 0.9797 - val_loss: 0.0248 - val_accuracy: 0.9942\n",
      "Epoch 8/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0382 - accuracy: 0.9862 - val_loss: 0.0122 - val_accuracy: 0.9956\n",
      "Epoch 9/25\n",
      "43/43 [==============================] - 57s 1s/step - loss: 0.0280 - accuracy: 0.9906 - val_loss: 0.0280 - val_accuracy: 0.9906\n",
      "Epoch 10/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0332 - accuracy: 0.9862 - val_loss: 0.0111 - val_accuracy: 0.9971\n",
      "Epoch 11/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0557 - accuracy: 0.9789 - val_loss: 0.0220 - val_accuracy: 0.9920\n",
      "Epoch 12/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0377 - accuracy: 0.9847 - val_loss: 0.0088 - val_accuracy: 0.9978\n",
      "Epoch 13/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0172 - accuracy: 0.9927 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0224 - accuracy: 0.9898 - val_loss: 0.0063 - val_accuracy: 0.9985\n",
      "Epoch 15/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0102 - accuracy: 0.9964 - val_loss: 0.0235 - val_accuracy: 0.9891\n",
      "Epoch 16/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0217 - accuracy: 0.9891 - val_loss: 0.0142 - val_accuracy: 0.9935\n",
      "Epoch 17/25\n",
      "43/43 [==============================] - 59s 1s/step - loss: 0.0171 - accuracy: 0.9942 - val_loss: 0.0158 - val_accuracy: 0.9927\n",
      "Epoch 18/25\n",
      "43/43 [==============================] - 60s 1s/step - loss: 0.0130 - accuracy: 0.9949 - val_loss: 0.0034 - val_accuracy: 0.9993\n",
      "Epoch 19/25\n",
      "43/43 [==============================] - 59s 1s/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0024 - val_accuracy: 0.9993\n",
      "Epoch 20/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "43/43 [==============================] - 59s 1s/step - loss: 0.0108 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "43/43 [==============================] - 59s 1s/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 7.2526e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "43/43 [==============================] - 59s 1s/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0028 - val_accuracy: 0.9993\n",
      "Epoch 24/25\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 4.8951e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "43/43 [==============================] - 59s 1s/step - loss: 0.0043 - accuracy: 0.9978 - val_loss: 0.0045 - val_accuracy: 0.9993\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "             \n",
    "history=model.fit_generator(train_batches,epochs=25,\n",
    "                           validation_data=test_batches,verbose=1,shuffle=True)       \n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('mask.h5')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.004516919143497944\n",
      "Test accuracy: 0.9992732405662537\n"
     ]
    }
   ],
   "source": [
    "# 任選一張圖片，例如戴口罩\n",
    "img_path = 'D:\\Python\\Python_3 機器學習應用開發 (主修)\\Class 8\\Homework\\HW3 CNN 神經網路實作_V2\\Lab 2\\data\\with_mask\\7-with-mask.jpg'\n",
    "# #載入圖檔，並縮放寬高為 (224, 224) \n",
    "# img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# #加一維，變成 (1, 224, 224, 3)\n",
    "# x = image.img_to_array(img)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "# x = preprocess_input(x)\n",
    "\n",
    "# Load Model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"mask.h5\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = model.evaluate(train_batches,verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.004516919143497944\n",
      "Test accuracy: 0.9992732405662537\n"
     ]
    }
   ],
   "source": [
    "# 任選一張圖片，例如沒戴口罩\n",
    "img_path = 'D:\\Python\\Python_3 機器學習應用開發 (主修)\\Class 8\\Homework\\HW3 CNN 神經網路實作_V2\\Lab 2\\data\\without_mask\\53.jpg'\n",
    "\n",
    "# Load Model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"mask.h5\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = model.evaluate(train_batches,verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟2：進行特徵工程，將特徵縮放成(0, 1)之間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet152V2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet_v2 import decode_predictions\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 images belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     27\u001b[0m normalization_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mRescaling(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m normalized_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: (normalization_layer(x), y))\n\u001b[0;32m     29\u001b[0m normalized_val_ds \u001b[38;5;241m=\u001b[39m test_batches\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: (normalization_layer(x), y))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define a function that applies Gaussian blur to an image\n",
    "def gaussian_blur(image):\n",
    "  return tf.image.random_gaussian_noise(image, mean=0.0, stddev=1.0)\n",
    "\n",
    "# Create an ImageDataGenerator\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=gaussian_blur,\n",
    "    vertical_flip=True)\n",
    "\n",
    "# Load images from a directory\n",
    "training_set = datagen.flow_from_directory('D:\\Python\\Python_3 機器學習應用開發 (主修)\\Class 8\\Homework\\HW3 CNN 神經網路實作_V2\\Lab 2\\data')\n",
    "\n",
    "# Define a generator function that yields batches of images and labels\n",
    "def generator():\n",
    "  for batch_x, batch_y in training_set:\n",
    "    yield (batch_x, batch_y)\n",
    "# Create a tf.data.Dataset from the generator function\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 256, 256, 3], [None]))\n",
    "\n",
    "\n",
    "#\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "normalized_ds = train_batches.map(lambda x, y: (normalization_layer(x), y))\n",
    "normalized_val_ds = test_batches.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示 ResNet152V2 完整的模型結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model = ResNet152V2(weights='imagenet')\n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟3：建立模型結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預先訓練好的模型 -- ResNet152V2\n",
    "base_model = ResNet152V2(weights='imagenet', include_top=False)\n",
    "print(base_model.summary())\n",
    "\n",
    "# 加上自訂的辨識層(Dense)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# 指定自訂的輸入層及辨識層(Dense)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# 模型前段不需訓練了\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 設定優化器(optimizer)、損失函數(loss)、效能衡量指標(metrics)的類別\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟5：模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定快取(cache)、prefetch，以增進訓練效率\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "normalized_ds = train_batches.cache().prefetch(buffer_size=AUTOTUNE) #prefetch: 先將下一批訓練資料取進來,以減少機器效能\n",
    "normalized_val_ds = test_batches.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 模型訓練\n",
    "history = model.fit(normalized_ds, validation_data = normalized_val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟6：繪製訓練時準確率/損失函數的變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對訓練過程的準確率繪圖\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['accuracy'], 'r', label='訓練準確率')\n",
    "plt.plot(history.history['val_accuracy'], 'g', label='驗證準確率')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('準確率')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='訓練損失')\n",
    "plt.plot(history.history['val_loss'], label = '驗證損失')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('損失')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟7：預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示辨識的類別\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任選一張圖片，例如玫瑰\n",
    "img_path = './images_test/rose.png'\n",
    "# 載入圖檔，並縮放寬高為 (224, 224) \n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# 加一維，變成 (1, 224, 224, 3)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# 預測\n",
    "preds = model.predict(x)\n",
    "\n",
    "# 顯示預測結果\n",
    "y_pred = [round(i * 100, 2) for i in preds[0]]\n",
    "print(f'預測機率(%)：{y_pred}')\n",
    "print(f'預測類別：{class_names[np.argmax(preds)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任選一張圖片，例如雛菊\n",
    "img_path = './images_test/daisy2.jpg'\n",
    "# 載入圖檔，並縮放寬高為 (224, 224) \n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# 加一維，變成 (1, 224, 224, 3)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# 預測\n",
    "preds = model.predict(x)\n",
    "\n",
    "# 顯示預測結果\n",
    "y_pred = [round(i * 100, 2) for i in preds[0]]\n",
    "print(f'預測機率(%)：{y_pred}')\n",
    "print(f'預測類別：{class_names[np.argmax(preds)]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
